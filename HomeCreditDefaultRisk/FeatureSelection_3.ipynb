{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411cbffa-4a9a-4b91-b74a-fd49acbf06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd #pandas class\n",
    "import numpy as np\n",
    "import traceback\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "import category_encoders as ce\n",
    "import time\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import precision_score,recall_score,confusion_matrix,f1_score,roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import logging\n",
    "plt.style.use('_mpl-gallery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3de6bd3-5be3-4933-9fe6-6785e7fff1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure logger\n",
    "log_filename=f\"/Users/chiragshah/Downloads/Projects/Data Science/home-credit-default-risk/logs/{(time.asctime()).replace(' ','_')}\"\n",
    "logging.basicConfig(filename=log_filename,filemode='a',format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4df6b84-01d2-40c1-a5c2-c6bb2866e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        \n",
    "        filepath=f'/Users/chiragshah/Downloads/Projects/Data Science/home-credit-default-risk/Cleaning_3.csv'\n",
    "        df=pd.read_csv(filepath)\n",
    "        df.drop(columns='Unnamed: 0',inplace=True)\n",
    "        #take backup of file\n",
    "        backup=copy.deepcopy(df)\n",
    "        \n",
    "except Exception as ex:\n",
    "    print(f'Following exception:\\n {ex}')\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e03042-d4d1-4e71-8075-a60262a3c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 158)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfb718e-1101-415a-a627-2c869ce6d11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24825, 158)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for no of positive examples\n",
    "df[df.iloc[:,0]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43d0fb-df54-4cff-abc3-1a5a71dbbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Firstly, we'll select all the features and use few tree-based models.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7dc01e-868d-4f69-8354-ee2abcf0cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Separate target from predictors.'''\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdfac58-4ce2-48f1-91ac-4356e017f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data in train, cross validation\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb68b7e-cf88-4de8-85cf-dfb2f791f816",
   "metadata": {},
   "source": [
    "The basic XGBoost model with no hyperparamter tuning.\n",
    "\n",
    "The suitable evaluation metrics for this analysis is AUC score because of binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff5f0fb3-81af-475f-ab7a-5bbaf86ffbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(X_train,X_val,y_train,y_val):\n",
    "    '''XGBoost model to fit and score various'''\n",
    "    \n",
    "    #define model\n",
    "    xgboost = GradientBoostingClassifier(random_state=0).fit(X_train,y_train)\n",
    "        \n",
    "    #predict\n",
    "    y_predict = xgboost.predict(X_val)\n",
    "    \n",
    "    #compute metric\n",
    "    auc_score = roc_auc_score(y_val,y_predict)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_predict).ravel() \n",
    "    \n",
    "    #print results\n",
    "    print('True negatives: ', tn, '\\nFalse positives: ', fp, '\\nFalse negatives: ', fn, '\\nTrue Positives: ', tp)\n",
    "    print('The AUC score is {}'.format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af576f5-cd7c-4b87-8ca4-142996d3a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negatives:  84763 \n",
      "False positives:  78 \n",
      "False negatives:  7322 \n",
      "True Positives:  91\n",
      "The AUC score is 0.5056781825034506\n"
     ]
    }
   ],
   "source": [
    "xgboost_model(X_train,X_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab3bd5-8a2d-4f18-85e2-fce4ddacaab5",
   "metadata": {},
   "source": [
    "The default xgboost model is pretty bad with only 91 TP were identified even with 50% percent score.\n",
    "\n",
    "In order to not to limit ourselves to any one ensemble methods, we will run another model random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85422afd-aa94-46ac-95d3-2ce56e816f21",
   "metadata": {},
   "source": [
    "Default Random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df30a269-da62-4df3-837d-3de3b9d35d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Random Forest classifier.'''\n",
    "def randomforest_model(X_train,X_val,y_train,y_val):\n",
    "    '''Random forest model to fit and score various'''\n",
    "\n",
    "    #define default model\n",
    "    random_forest=RandomForestClassifier(random_state=0).fit(X_train,y_train)\n",
    "    \n",
    "    #predict\n",
    "    y_predict = random_forest.predict(X_val)\n",
    "    \n",
    "    #compute metric\n",
    "    auc_score = roc_auc_score(y_val,y_predict)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_predict).ravel() \n",
    "    \n",
    "    #print results\n",
    "    print('True negatives: ', tn, '\\nFalse positives: ', fp, '\\nFalse negatives: ', fn, '\\nTrue Positives: ', tp)\n",
    "    print('The AUC score is {}'.format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64a69051-c5dc-4ac2-b966-de2adcc74327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negatives:  84836 \n",
      "False positives:  5 \n",
      "False negatives:  7404 \n",
      "True Positives:  9\n",
      "The AUC score is 0.5005775747984145\n"
     ]
    }
   ],
   "source": [
    "randomforest_model(X_train,X_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261f64c-1157-44e8-9f56-ef5a621bb714",
   "metadata": {},
   "source": [
    "Random forest model is also not good in identifying the true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf388f48-a324-4169-a6bf-665ad86633b6",
   "metadata": {},
   "source": [
    "Before we perform hyperparameter tunning it appear our positive examples are very less. It might be a good idea to perform oversampling so that our model can learn the\n",
    "positive examples better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e43f30-7f19-4cf1-99b3-1afe1d4b9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "#fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "\n",
    "#split the data again with oversampled dataset.\n",
    "X_train_over,X_val_over,y_train_over,y_val_over = train_test_split(X_over,y_over,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aeddc3-6e52-4774-bf42-77ab87a1174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We'll call both our models again with oversampled dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcd9554b-716f-469c-818a-98664352b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negatives:  58211 \n",
      "False positives:  26634 \n",
      "False negatives:  26042 \n",
      "True Positives:  58725\n",
      "The AUC score is 0.689433890908956\n"
     ]
    }
   ],
   "source": [
    "#XGBoost with oversampled dataset.\n",
    "xgboost_model(X_train_over,X_val_over,y_train_over,y_val_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc888680-5445-48a0-8588-bc98b9752533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost with oversampling increased our model's AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7971e7e-b1c8-417c-8dbf-49842af0be47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negatives:  84399 \n",
      "False positives:  446 \n",
      "False negatives:  23 \n",
      "True Positives:  84744\n",
      "The AUC score is 0.9972360117632368\n"
     ]
    }
   ],
   "source": [
    "#Random Forest with oversampled dataset.\n",
    "randomforest_model(X_train_over,X_val_over,y_train_over,y_val_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c31fd-d7d3-4782-9ba9-be20e91c066c",
   "metadata": {},
   "source": [
    "Clearly, oversampling our dataset is performing really well, even without any hyperparameter tunning.\n",
    "\n",
    "The default XGBoost with oversample gave us any output of 0.69 AUC score, whereas the Random Forest performed with 0.99 percent. The number of true positives are also\n",
    "really high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f4d58-fd9f-4cfb-8fd5-9765cd5fd663",
   "metadata": {},
   "source": [
    "So, now the question is out of 157 predictors how many are important to train our model and what are irrelavant.\n",
    "We'll try to check the feature importance in our model to identify the best subset of predictors to achieve global optimum.\n",
    "\n",
    "For this we'll use a greedy feature selection approach called RFE (Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785b3377-e7e4-4ce9-aa4d-e77a3cb6038a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Random Forest with recursive feature elimination.\n",
    "'''\n",
    "def compute_metrics(model,X_train,y_train,X_val,y_val):\n",
    "    '''Function to compute test and validation accuracy'''\n",
    "    \n",
    "    logging.info('Execution start compute_metrics')\n",
    "    \n",
    "    f_score = 0 \n",
    "    \n",
    "    #calculate accuracy\n",
    "    train_accuracy_score = model.score(X_train,y_train)\n",
    "    val_accuracy_score = model.score(X_val,y_val)\n",
    "    logging.info('Train accuracy achieved is {}'.format(train_accuracy_score))\n",
    "    logging.info('Validation accuracy achieved is {}'.format(val_accuracy_score))\n",
    "    \n",
    "    #calculate TP, TN, FP, FN\n",
    "    prediction = model.predict(X_val)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, prediction).ravel()\n",
    "    logging.info(f'True negatives: {tn}, False positives: {fp}, False negatives: {fn}, True Positives: {tp}')\n",
    "    \n",
    "    #calculate f1_score\n",
    "    f_score = f1_score(y_val,prediction)\n",
    "    logging.info('F1 Score: {}'.format(f_score))\n",
    "    \n",
    "    logging.info('Execution end compute_metrics')\n",
    "    \n",
    "    #calculate auc_score\n",
    "    auc_score = roc_auc_score(y_val,prediction)\n",
    "    logging.info('AUC score: {}'.format(auc_score))\n",
    "    \n",
    "    return auc_score\n",
    "    \n",
    "def compute_permutation_importance(model,X_val,y_val):\n",
    "    '''Calculate permuation importance of model's predictors.\n",
    "        Reason for choosing this method is because the predictors are of various data types.\n",
    "    '''\n",
    "    \n",
    "    logging.info('Execution start compute_permutation_importance function')\n",
    "    \n",
    "    #train_result = permutation_importance(model,X_train,y_train,n_repeats=5,n_jobs=2,random_state=42)\n",
    "    val_result = permutation_importance(model,X_val,y_val,n_repeats=5,n_jobs=2,random_state=42)\n",
    "    \n",
    "    sorted_idx = val_result.importances_mean.argsort()\n",
    "    \n",
    "    #train_importances = pd.DataFrame(train_result.imp[sorted_idx].T,columns=X_train.column[sorted_idx])\n",
    "    test_importances = pd.DataFrame(val_result.importances[sorted_idx].T,columns=X_val.columns[sorted_idx])\n",
    "    \n",
    "    logging.info('Execution end compute_permutation_importance function.')\n",
    "    \n",
    "    return test_importances\n",
    "\n",
    "def recursive_feature_selection(X_over,y_over):\n",
    "    '''The function will employ a feature selection technique names RFE to identify minimun subset of features that can achieve\n",
    "        our objective and make prediction with acceptable accuracy.\n",
    "    '''\n",
    "    \n",
    "    logging.info('Execution start recursive_feature_selection function.')\n",
    "    \n",
    "    #declare variables\n",
    "    assessment_sheet = {}\n",
    "    sampling_count = 10\n",
    "    rfe_df = copy.deepcopy(df)\n",
    "    \n",
    "    logging.info(f'Initial dataframe predictors shape: {X_over.shape}, and response shape: {y_over.shape}')\n",
    "\n",
    "    while sampling_count > 0:\n",
    "        \n",
    "        auc_score = 0\n",
    "        logging.info('-'*180)\n",
    "        \n",
    "        #assign predictors and response to X, y for simplicity\n",
    "        X = X_over.sample(frac = 0.5, random_state = 42, axis = 1)\n",
    "        y =  y_over\n",
    "        logging.info(f'Predictors shape: {X.shape}, Response shape: {y.shape}')\n",
    "    \n",
    "        #Split the data in train, cross validation.\n",
    "        X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.30,random_state=42)\n",
    "                    \n",
    "        num_of_predictors = X_train.shape[1]\n",
    "        while num_of_predictors > 25:\n",
    "            #define default model\n",
    "            random_forest = RandomForestClassifier(random_state=0).fit(X_train,y_train)\n",
    "    \n",
    "            #compute evaluation metrics\n",
    "            auc_score = compute_metrics(random_forest,X_train,y_train,X_val,y_val)\n",
    "        \n",
    "            #print the auc score with current subset of predictors.\n",
    "            logging.info('Current subset of predictors are: {}'.format(list(X.columns)))\n",
    "        \n",
    "            #compute permuation importance\n",
    "            test_importances = compute_permutation_importance(random_forest,X_val,y_val)\n",
    "    \n",
    "            #identify the least importance feature\n",
    "            lst_imp=np.mean(test_importances,axis=0).sort_values().head(1)\n",
    "            lst_imp_index,lst_imp_value = list(lst_imp.index)[0],lst_imp.iloc[0]\n",
    "            logging.info(f'The least important predictor found in this iteration is: {lst_imp_index}, Value: {lst_imp_value}')  \n",
    "    \n",
    "            #remove the least important predictor\n",
    "            X.drop(columns = lst_imp_index,inplace = True)\n",
    "            logging.info(f'Dropped the least important predictor from this iteration.')\n",
    "            logging.info(f'Current shape of predictors: {X.shape}')\n",
    "        \n",
    "            #assign remaining num of predictors.\n",
    "            num_of_predictors = X.shape[1]\n",
    "        \n",
    "        sampling_count -= 1\n",
    "    \n",
    "    \n",
    "    logging.info(f'Remaining predictors are {list(sampled_df.columns)}')\n",
    "    logging.info('Execution end recursive_feature_selection function.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3210bf8-9c4a-4ae7-b946-c86c912f3d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process end\n"
     ]
    }
   ],
   "source": [
    "recursive_feature_selection(X_over,y_over)\n",
    "print('Process end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c2920-7b3c-4d04-8447-218c6bcef0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
